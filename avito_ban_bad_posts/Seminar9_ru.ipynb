{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование глубокого обучения в NLP\n",
    "\n",
    "Смотрите в этой серии:\n",
    " * Простые способы работать с текстом, bag of words\n",
    " * Word embedding и... нет, это не word2vec\n",
    " * Как сделать лучше? Текстовые свёрточные сети\n",
    " * Совмещение нескольких различных источников данных\n",
    " * Решение +- реальной задачи нейронками \n",
    " \n",
    "За помощь в организации свёрточной части спасибо Ирине Гольцман"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "Для работы этого семинара вам потреюуется nltk v3.2\n",
    "\n",
    "__Важно, что именно v3.2, чтобы правильно работал токенизатор__\n",
    "\n",
    "Устаовить/обновиться до неё можно командой\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* Если у вас старый pip, предварительно нужно сделать `sudo pip install --upgrade pip`\n",
    "\n",
    "Если у вас нет доступа к этой версии - просто убедитесь, что токены в token_counts включают русские слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Для людей со слабым ПК\n",
    " * Этот семинар можно выполнить, имея относительно скромную машину (<= 4Gb RAM) \n",
    " * Для этого существует специальный флаг \"low_RAM_mode\" - если он True, семинар работает в режиме экономии вашей памяти\n",
    " * Если у вас 8GB и больше - проблем с памятью возникнуть не должно\n",
    " * Если включить режим very_low_ram, расход мамяти будет ещё меньше, но вам может быть более трудно научить нейронку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #если у вас меньше 3GB оперативки, включите оба флага"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/anotn/anaconda2/bin/pip\", line 4, in <module>\r\n",
      "    import pip\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/__init__.py\", line 14, in <module>\r\n",
      "    from pip.utils import get_installed_distributions, get_prog\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/utils/__init__.py\", line 27, in <module>\r\n",
      "    from pip._vendor import pkg_resources\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2927, in <module>\r\n",
      "    @_call_aside\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2913, in _call_aside\r\n",
      "    f(*args, **kwargs)\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2940, in _initialize_master_working_set\r\n",
      "    working_set = WorkingSet._build_master()\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 626, in _build_master\r\n",
      "    ws = cls()\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 619, in __init__\r\n",
      "    self.add_entry(entry)\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 675, in add_entry\r\n",
      "    for dist in find_distributions(entry, True):\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1982, in find_on_path\r\n",
      "    path_item, entry, metadata, precedence=DEVELOP_DIST\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2370, in from_location\r\n",
      "    py_version=py_version, platform=platform, **kw\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2687, in _reload_version\r\n",
      "    md_version = _version_from_file(self._get_metadata(self.PKG_INFO))\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2335, in _version_from_file\r\n",
      "    line = next(iter(version_lines), '')\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2503, in _get_metadata\r\n",
      "    for line in self.get_metadata_lines(name):\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1477, in get_metadata_lines\r\n",
      "    return yield_lines(self.get_metadata(name))\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1469, in get_metadata\r\n",
      "    return self._get(self._fn(self.egg_info, name))\r\n",
      "  File \"/home/anotn/.local/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 1581, in _get\r\n",
      "    return stream.read()\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Познакомимся с данными\n",
    "\n",
    "Бывший kaggle-конкурс про выявление нежелательного контента.\n",
    "\n",
    "Описание конкурса есть тут - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Скачать\n",
    "Если много RAM,\n",
    " * Из данных конкурса (вкладка Data) нужно скачать avito_train.tsv и распаковать в папку с тетрадкой\n",
    "Если мало RAM,\n",
    " * Cкачайте прореженную выборку отсюда \n",
    "     * Пожатая https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * Непожатая https://yadi.sk/d/I1v7mZ6Sqw2WK\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Много разных признаков:\n",
    "* 2 вида текста - заголовок и описание\n",
    "* Много специальных фичей - цена, количество телефонов/ссылок/e-mail адресов\n",
    "* Категория и субкатегория - как ни странно, категориальные фичи\n",
    "* Аттрибуты - много категориальных признаков\n",
    "\n",
    "Нужно предсказать всего 1 бинарный признак - есть ли в рекламе нежелательный контент.\n",
    "* Под нежелательным контентом понимается криминал, прон, афера, треска и прочие любимые нами темы.\n",
    "* Да, если присмотреться к заблокированным объявлениям, можно потерять аппетит и сон на пару дней.\n",
    "* Однако профессия аналитика данных обязывает вас смотреть на данные.\n",
    " * А кто сказал, что будет легко? Data Science - опасная профессия.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # Если у вас много оперативки\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #Если у вас меньше 4gb оперативки\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля заблокированных объявлений 0.228222107326\n",
      "Всего объявлений: 1204949\n"
     ]
    }
   ],
   "source": [
    "print \"Доля заблокированных объявлений\",df.is_blocked.mean()\n",
    "print \"Всего объявлений:\",len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбалансируем выборку\n",
    "* Выборка смещена в сторону незаблокированных объявлений\n",
    " * 4 миллиона объявлений и только 250 тысяч заблокированы.\n",
    " * Давайте просто выберем случайные 250 тысяч незаблокированных объявлений и сократим выборку до полумилиона.\n",
    " * В последствии можно испоьзовать более умные способы сбалансировать выборку\n",
    "\n",
    "\n",
    "__Если у вас слабый ПК и вы видите OutOfMemory, попробуйте уменьшить размер выборки до 100 000 примеров__\n",
    "\n",
    "__Алсо если вы не хотите ждать чтения всех данных каждый раз - сохраните уменьшенную выборку и читайте её__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля заблокированных объявлений: 0.5\n",
      "Всего объявлений: 549992\n"
     ]
    }
   ],
   "source": [
    "dfunblocked = df[df.is_blocked == 0]\n",
    "dfblocked = df[df.is_blocked == 1]\n",
    "\n",
    "df = pd.concat([dfblocked,dfunblocked[:len(dfblocked)]])\n",
    "print \"Доля заблокированных объявлений:\",df.is_blocked.mean()\n",
    "print \"Всего объявлений:\",len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Токенизируем примеры\n",
    "\n",
    "Сначала соберём словарь всех возможных слов.\n",
    "Поставим каждому слову в соответствие целое число - его id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#словарь для всех токенов\n",
    "token_counts = Counter()\n",
    "\n",
    "#все заголовки и описания\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#считаем частоты слов\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вырежем редкие токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noSFJ6vb/ATLCti11vlRYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3014152d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#распределение частот слов - большинство слов встречаются очень редко - для нас это мусор\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#возьмём только те токены, которые встретились хотя бы 10 раз в обучающей выборке\n",
    "#информацию о том, сколько раз встретился каждый токен, можно найти в словаре token_counts\n",
    "tokens= {}\n",
    "min_count = 10\n",
    "for token in token_counts.keys():\n",
    "    if(token_counts[token]>=min_count):\n",
    "        tokens[token] = token_counts[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего токенов: 87867\n",
      "Алярм! Много токенов. Если вы знаете, что делаете - всё ок, если нет - возможно, вы слижком слабо обрезали токены по количеству\n"
     ]
    }
   ],
   "source": [
    "print \"Всего токенов:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Алярм! Мало токенов. Проверьте, есть ли в token_to_id юникодные символы, если нет - обновите nltk или возьмите другой токенизатор\"\n",
    "if len(token_to_id) < 1000000:\n",
    "    print \"Алярм! Много токенов. Если вы знаете, что делаете - всё ок, если нет - возможно, вы слижком слабо обрезали токены по количеству\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заменим слова на их id\n",
    "Для каждого описания установим максимальную длину. \n",
    " * Если описание больше длины - обрежем, если меньше - дополним нулями.\n",
    " * Таким образом, у нас получится матрица размера (число объявлений)x(максимальная длина)\n",
    " * Элемент под индексами i,j - номер j-того слова i-того объявления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Пример формата данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 15)\n",
      "Поездки на таможню, печать в паспорте -> [67547 58372 45077  6585 55807 68875     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [32906     0 33296     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [27004 44608     0 14256 52206     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Как вы видите, всё довольно грязно. Посмотрим, сожрёт ли это нейронка __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нетекстовые признаки\n",
    "\n",
    "Часть признаков не являются строками текста: цена, количество телефонов, категория товара.\n",
    "\n",
    "Их можно обработать отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Возьмём числовые признаки\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Возьмём one-hot encoding категорий товара.\n",
    "#Для этого можно использовать DictVectorizer (или другой ваш любимый препроцессор)\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "for cat_str, subcat_str in df[[\"category\",\"subcategory\"]].values:\n",
    "    \n",
    "    cat_dict = {\"category\":cat_str,\"subcategory\":subcat_str}\n",
    "    categories.append(cat_dict)\n",
    "    \n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поделим данные на обучение и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#целевая переменная - есть заблокирован ли контент\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#закодированное название\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#закодированное описание\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#все нетекстовые признаки\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#поделим всё это на обучение и тест\n",
    "from sklearn.cross_validation import train_test_split\n",
    "data_tuple = train_test_split(title_tokens,desc_tokens,df_non_text.values,target)\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраним данные [опционально] \n",
    "\n",
    "* В этот момент вы можете сохранить все НУЖНЫЕ данные на диск и перезапусатить тетрадку, после чего считать их - чтобы выкинуть всё ненужное.\n",
    " * рекомендуется, если у вас мало памяти\n",
    "* Для этого нужно один раз выполнить эту клетку с save_prepared_data=True. После этого можно начинать тетрадку с ЭТОЙ табы в режиме read_prepared_data=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохраняем подготовленные данные... (может занять до 3 минут)\n",
      "готово\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_prepared_data = True #сохранить\n",
    "read_prepared_data = False #cчитать\n",
    "\n",
    "#за 1 раз данные можно либо записать, либо прочитать, но не и то и другое вместе\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Сохраняем подготовленные данные... (может занять до 3 минут)\"\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"готово\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Читаем сохранённые данные...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "\n",
    "\n",
    "        \n",
    "    #повторно импортируем библиотеки, чтобы было удобно перезапускать тетрадку с этой клетки\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "        \n",
    "    print \"готово\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поучим нейронку\n",
    "\n",
    "Поскольку у нас есть несколько источников данных, наша нейронная сеть будет немного отличаться от тех, что вы тренировали раньше.\n",
    "\n",
    "* Отдельный вход для заголовка\n",
    " * свёртка + global max pool или RNN\n",
    "* Отдельный вход для описания\n",
    " * свёртка + global max pool или RNN\n",
    "* Отдельный вход для категориальных признаков\n",
    " * обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "Всё это нужно как-то смешать - например, сконкатенировать\n",
    "\n",
    "* Выход - обычный двухклассовый выход\n",
    " * 1 сигмоидальный нейрон и binary_crossentropy\n",
    " * 2 нейрона с softmax и categorical_crossentropy - то же самое, что 1 сигмоидальный\n",
    " * 1 нейрон без нелинейности (lambda x: x) и hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#загрузим библиотеки\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 входа и 1 выход\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Описание\n",
    "first_embeding = lasagne.layers.EmbeddingLayer(descr_inp,input_size=len(token_to_id)+1,output_size=128, W=lasagne.init.Normal())\n",
    "\n",
    "#поменять порядок осей с [batch, time, unit] на [batch,unit,time], чтобы свёртки шли по оси времени, а не по нейронам\n",
    "descr_nn = lasagne.layers.DimshuffleLayer(first_embeding, [0,2,1])\n",
    "# 1D свёртка на ваш вкус\n",
    "descr_nn = lasagne.layers.Conv1DLayer(descr_nn,num_filters=10, filter_size=3)\n",
    "descr_nn = lasagne.layers.batch_norm(descr_nn)\n",
    "\n",
    "descr_nn = lasagne.layers.MaxPool1DLayer(descr_nn,pool_size = 10)\n",
    "descr_nn = lasagne.layers.batch_norm(descr_nn)\n",
    "\n",
    "descr_nn = lasagne.layers.Conv1DLayer(descr_nn,num_filters=10, filter_size=3)\n",
    "descr_nn = lasagne.layers.batch_norm(descr_nn)\n",
    "# максимум по времени для каждого нейрона\n",
    "descr_nn = lasagne.layers.GlobalPoolLayer(descr_nn,pool_function=T.max)\n",
    "descr_nn = lasagne.layers.batch_norm(descr_nn)\n",
    "\n",
    "descr_nn = lasagne.layers.DenseLayer(descr_nn, num_units = 10, nonlinearity = lasagne.nonlinearities.sigmoid)\n",
    "descr_nn = lasagne.layers.batch_norm(descr_nn)\n",
    "\n",
    "#А ещё можно делать несколько параллельных свёрток разного размера или стандартный пайплайн \n",
    "#1dconv -> 1d max pool ->1dconv и в конце global pool \n",
    "\n",
    "\n",
    "# Заголовок\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1,output_size=128, W=first_embeding.W)\n",
    "\n",
    "title_nn = lasagne.layers.DimshuffleLayer(title_nn, [0,2,1])\n",
    "\n",
    "title_nn = lasagne.layers.Conv1DLayer(title_nn, num_filters=10, filter_size=3)\n",
    "title_nn = lasagne.layers.batch_norm(title_nn)\n",
    "\n",
    "title_nn = lasagne.layers.GlobalPoolLayer(title_nn, pool_function=T.max)\n",
    "title_nn = lasagne.layers.batch_norm(title_nn)\n",
    "\n",
    "title_nn = lasagne.layers.DenseLayer(title_nn, num_units = 15, nonlinearity = lasagne.nonlinearities.softmax)\n",
    "title_nn = lasagne.layers.batch_norm(title_nn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Нетекстовые признаки\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units = 10, nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='Params')\n",
    "\n",
    "cat_nn = lasagne.layers.batch_norm(cat_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.ConcatLayer([descr_nn, title_nn, cat_nn])                         \n",
    "nn = lasagne.layers.batch_norm(nn)\n",
    "nn = lasagne.layers.DenseLayer(nn,1024)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Целевая функция и обновления весов\n",
    "\n",
    "* Делаем всё стандартно:\n",
    " * получаем предсказание\n",
    " * считаем функцию потерь\n",
    " * вычисляем обновления весов\n",
    " * компилируем итерацию обучения и оценки весов\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * Важный параметр - delta - насколько глубоко пример должен быть в правильном классе, чтобы перестать нас волновать\n",
    " * В описании функции в документации может быть что-то про ограничения на +-1 - не верьте этому - главное, чтобы в функции по умолчанию стоял флаг `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Все обучаемые параметры сети\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Обычное предсказание нейронки\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#функция потерь для prediction\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 1.0).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Шаг оптимизации весов\n",
    "updates = lasagne.updates.adagrad(loss,weights,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтобы оценивать качество сети, в которой есть элемент случайности \n",
    " * Dropout, например,\n",
    " * Нужно отдельно вычислить ошибку для случая, когда dropout выключен (deterministic = True)\n",
    " * К слову, неплохо бы убедиться, что droput нам вообще нужен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Предсказание нейронки без учёта dropout и прочего шума - если он есть\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#функция потерь для det_prediction\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction,target_y,delta = 1.0).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скомпилируем функции обучения и оценки качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Главный цикл обучения\n",
    "* Всё как обычно - в цикле по минибатчам запускаем функцию обновления весов.\n",
    "* Поскольку выборка огромна, а чашки чая хватает в среднем на  100к примеров, будем на каждой эпохе пробегать только часть примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# наш старый знакомый - итератор по корзинкам - теперь умеет работать с произвольным числом каналов (название, описание, категории, таргет)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    \n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что можно покрутить?\n",
    "\n",
    "* batch_size - сколько примеров обрабатывается за 1 раз\n",
    "  * Чем больше, тем оптимизация стабильнее, но тем и медленнее на начальном этапе\n",
    "  * Возможно имеет смысл увеличивать этот параметр на поздних этапах обучения\n",
    "* minibatches_per_epoch - количество минибатчей, после которых эпоха принудительно завершается\n",
    "  * Не влияет на обучение - при малых значениях просто будет чаще печататься отчёт\n",
    "  * Ставить 10 или меньше имеет смысл только для того, чтобы убедиться, что ваша сеть не упала с ошибкой\n",
    "* n_epochs - сколько всего эпох сеть будет учиться\n",
    "  * Никто не отменял `n_epochs = 10**10` и остановку процесса вручную по возвращению с дачи/из похода. \n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* Если вы выставили небольшой minibatches_per_epoch, качество сети может сильно скакать возле 0.5 на первых итерациях, пока сеть почти ничему не научилась.\n",
    "\n",
    "* На первых этапах попытки стоит сравнивать в первую очередь по AUC, как по самой стабильной метрике.\n",
    "\n",
    "* Метрика Average Precision at top 2.5% (APatK) - сама по себе очень нестабильная на маленьких выборках, поэтому её имеет смысл оценивать на на всех примерах (см. код ниже). Для менее, чем 10000 примеров она вовсе неинформативна.\n",
    "\n",
    "* Для сравнения методов оптимизации и регуляризаторов будет очень полезно собирать метрики качества после каждой итерации и строить график по ним после обучения\n",
    "\n",
    "* Как только вы убедились, что сеть не упала - имеет смысл дать ей покрутиться - на стандартном ноутбуке хотя бы пару часов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 0.842927816336\n",
      "\tacc: 0.779900990099\n",
      "\tauc: 0.84520567648\n",
      "\tap@k: 0.454454404238\n",
      "Val:\n",
      "\tloss: 4.58585538529\n",
      "\tacc: 0.503564356436\n",
      "\tauc: 0.729704001941\n",
      "\tap@k: 0.834162808209\n",
      "Train:\n",
      "\tloss: 0.267176145403\n",
      "\tacc: 0.88801980198\n",
      "\tauc: 0.95301972494\n",
      "\tap@k: 0.991331104525\n",
      "Val:\n",
      "\tloss: 0.294621501152\n",
      "\tacc: 0.886831683168\n",
      "\tauc: 0.947776326406\n",
      "\tap@k: 0.999708882187\n",
      "Train:\n",
      "\tloss: 0.220840964094\n",
      "\tacc: 0.908316831683\n",
      "\tauc: 0.963016523749\n",
      "\tap@k: 0.992750841512\n",
      "Val:\n",
      "\tloss: 0.348482039279\n",
      "\tacc: 0.858514851485\n",
      "\tauc: 0.954922150309\n",
      "\tap@k: 0.937290186463\n",
      "Train:\n",
      "\tloss: 0.180277068607\n",
      "\tacc: 0.922772277228\n",
      "\tauc: 0.971654572232\n",
      "\tap@k: 0.996007197465\n",
      "Val:\n",
      "\tloss: 0.276009409445\n",
      "\tacc: 0.894059405941\n",
      "\tauc: 0.958977463775\n",
      "\tap@k: 0.993996869927\n",
      "Train:\n",
      "\tloss: 0.15699188401\n",
      "\tacc: 0.934158415842\n",
      "\tauc: 0.973626459119\n",
      "\tap@k: 0.999321238242\n",
      "Val:\n",
      "\tloss: 0.209993959906\n",
      "\tacc: 0.920396039604\n",
      "\tauc: 0.96841961732\n",
      "\tap@k: 0.990088707222\n",
      "Train:\n",
      "\tloss: 0.171123843577\n",
      "\tacc: 0.929207920792\n",
      "\tauc: 0.972013313153\n",
      "\tap@k: 0.986891621418\n",
      "Val:\n",
      "\tloss: 0.266946805038\n",
      "\tacc: 0.893663366337\n",
      "\tauc: 0.957189919802\n",
      "\tap@k: 0.962002713934\n",
      "Train:\n",
      "\tloss: 0.145971827594\n",
      "\tacc: 0.936633663366\n",
      "\tauc: 0.978642259822\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.208223921535\n",
      "\tacc: 0.922772277228\n",
      "\tauc: 0.965645473613\n",
      "\tap@k: 0.995479633502\n",
      "Train:\n",
      "\tloss: 0.142502744597\n",
      "\tacc: 0.938811881188\n",
      "\tauc: 0.976985493316\n",
      "\tap@k: 0.995704180254\n",
      "Val:\n",
      "\tloss: 0.175838031328\n",
      "\tacc: 0.927821782178\n",
      "\tauc: 0.972491765481\n",
      "\tap@k: 0.997396838994\n",
      "Train:\n",
      "\tloss: 0.143198608795\n",
      "\tacc: 0.937920792079\n",
      "\tauc: 0.975484306935\n",
      "\tap@k: 0.994882759289\n",
      "Val:\n",
      "\tloss: 0.247023355776\n",
      "\tacc: 0.911881188119\n",
      "\tauc: 0.966923282844\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.124499314319\n",
      "\tacc: 0.944158415842\n",
      "\tauc: 0.98025466087\n",
      "\tap@k: 0.986027240771\n",
      "Val:\n",
      "\tloss: 0.177193432287\n",
      "\tacc: 0.931188118812\n",
      "\tauc: 0.975288816999\n",
      "\tap@k: 0.995465992747\n",
      "Train:\n",
      "\tloss: 0.133640780555\n",
      "\tacc: 0.942871287129\n",
      "\tauc: 0.976771962829\n",
      "\tap@k: 0.997610703661\n",
      "Val:\n",
      "\tloss: 0.154760856003\n",
      "\tacc: 0.931584158416\n",
      "\tauc: 0.974353425255\n",
      "\tap@k: 0.974946204188\n",
      "Train:\n",
      "\tloss: 0.131651771745\n",
      "\tacc: 0.941188118812\n",
      "\tauc: 0.978187274902\n",
      "\tap@k: 0.999624059981\n",
      "Val:\n",
      "\tloss: 0.230924211899\n",
      "\tacc: 0.911881188119\n",
      "\tauc: 0.969034578163\n",
      "\tap@k: 0.998870707001\n",
      "Train:\n",
      "\tloss: 0.120936751425\n",
      "\tacc: 0.947524752475\n",
      "\tauc: 0.980417743768\n",
      "\tap@k: 0.996141368951\n",
      "Val:\n",
      "\tloss: 0.200971753804\n",
      "\tacc: 0.924257425743\n",
      "\tauc: 0.968203580466\n",
      "\tap@k: 0.998155431194\n",
      "Train:\n",
      "\tloss: 0.116577675709\n",
      "\tacc: 0.948217821782\n",
      "\tauc: 0.983485341908\n",
      "\tap@k: 0.998952841079\n",
      "Val:\n",
      "\tloss: 0.141064878077\n",
      "\tacc: 0.938514851485\n",
      "\tauc: 0.978237946761\n",
      "\tap@k: 0.996173373828\n",
      "Train:\n",
      "\tloss: 0.113337939182\n",
      "\tacc: 0.94900990099\n",
      "\tauc: 0.981807372861\n",
      "\tap@k: 0.993715364689\n",
      "Val:\n",
      "\tloss: 0.148368153259\n",
      "\tacc: 0.935148514851\n",
      "\tauc: 0.977208642219\n",
      "\tap@k: 0.982038822095\n",
      "Train:\n",
      "\tloss: 0.109672725013\n",
      "\tacc: 0.950891089109\n",
      "\tauc: 0.981994415663\n",
      "\tap@k: 0.999112159807\n",
      "Val:\n",
      "\tloss: 0.134748025127\n",
      "\tacc: 0.942079207921\n",
      "\tauc: 0.97990409173\n",
      "\tap@k: 0.982539235549\n",
      "Train:\n",
      "\tloss: 0.112783145881\n",
      "\tacc: 0.948910891089\n",
      "\tauc: 0.980568862899\n",
      "\tap@k: 0.994590161136\n",
      "Val:\n",
      "\tloss: 0.148583749052\n",
      "\tacc: 0.939801980198\n",
      "\tauc: 0.97323943325\n",
      "\tap@k: 0.961785417726\n",
      "Train:\n",
      "\tloss: 0.106591996971\n",
      "\tacc: 0.953861386139\n",
      "\tauc: 0.979527803774\n",
      "\tap@k: 0.982585933188\n",
      "Val:\n",
      "\tloss: 0.155119562031\n",
      "\tacc: 0.938613861386\n",
      "\tauc: 0.979945223773\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.114758690126\n",
      "\tacc: 0.949405940594\n",
      "\tauc: 0.979753147568\n",
      "\tap@k: 0.986140075223\n",
      "Val:\n",
      "\tloss: 0.145442176987\n",
      "\tacc: 0.935148514851\n",
      "\tauc: 0.979547513587\n",
      "\tap@k: 0.98786637563\n",
      "Train:\n",
      "\tloss: 0.102028483075\n",
      "\tacc: 0.95396039604\n",
      "\tauc: 0.982106570595\n",
      "\tap@k: 0.984087429897\n",
      "Val:\n",
      "\tloss: 0.144559009597\n",
      "\tacc: 0.93495049505\n",
      "\tauc: 0.978461185549\n",
      "\tap@k: 0.976764276746\n",
      "Train:\n",
      "\tloss: 0.108262806384\n",
      "\tacc: 0.950099009901\n",
      "\tauc: 0.981551569833\n",
      "\tap@k: 0.990961699471\n",
      "Val:\n",
      "\tloss: 0.168276223287\n",
      "\tacc: 0.930792079208\n",
      "\tauc: 0.967773272487\n",
      "\tap@k: 0.974646260202\n",
      "Train:\n",
      "\tloss: 0.0999458157188\n",
      "\tacc: 0.953564356436\n",
      "\tauc: 0.984093059913\n",
      "\tap@k: 0.99609976296\n",
      "Val:\n",
      "\tloss: 0.158667524316\n",
      "\tacc: 0.927623762376\n",
      "\tauc: 0.97871275631\n",
      "\tap@k: 0.98244382352\n",
      "Train:\n",
      "\tloss: 0.101897389067\n",
      "\tacc: 0.954554455446\n",
      "\tauc: 0.982579404291\n",
      "\tap@k: 0.995366564881\n",
      "Val:\n",
      "\tloss: 0.136827896492\n",
      "\tacc: 0.941782178218\n",
      "\tauc: 0.982598529556\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.10243279918\n",
      "\tacc: 0.952871287129\n",
      "\tauc: 0.984920850897\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.119219735183\n",
      "\tacc: 0.945346534653\n",
      "\tauc: 0.983358831458\n",
      "\tap@k: 0.996738054297\n",
      "Train:\n",
      "\tloss: 0.0952431265399\n",
      "\tacc: 0.956435643564\n",
      "\tauc: 0.984914557061\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.13108816375\n",
      "\tacc: 0.943663366337\n",
      "\tauc: 0.980601657034\n",
      "\tap@k: 0.978227790097\n",
      "Train:\n",
      "\tloss: 0.0956185899339\n",
      "\tacc: 0.957623762376\n",
      "\tauc: 0.983277423836\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.128558200898\n",
      "\tacc: 0.944455445545\n",
      "\tauc: 0.984597166759\n",
      "\tap@k: 0.986470270152\n",
      "Train:\n",
      "\tloss: 0.0989148140986\n",
      "\tacc: 0.955544554455\n",
      "\tauc: 0.983611341654\n",
      "\tap@k: 0.996858551501\n",
      "Val:\n",
      "\tloss: 0.13373476347\n",
      "\tacc: 0.94396039604\n",
      "\tauc: 0.982210690865\n",
      "\tap@k: 0.975916938654\n",
      "Train:\n",
      "\tloss: 0.0984349029584\n",
      "\tacc: 0.95504950495\n",
      "\tauc: 0.984673760248\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.138069979712\n",
      "\tacc: 0.939405940594\n",
      "\tauc: 0.98140255847\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.0937012794669\n",
      "\tacc: 0.957920792079\n",
      "\tauc: 0.984660561666\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.161415579003\n",
      "\tacc: 0.937524752475\n",
      "\tauc: 0.977200652338\n",
      "\tap@k: 0.988105259059\n",
      "Train:\n",
      "\tloss: 0.0950689488109\n",
      "\tacc: 0.957722772277\n",
      "\tauc: 0.982109025386\n",
      "\tap@k: 0.997539514836\n",
      "Val:\n",
      "\tloss: 0.132882739727\n",
      "\tacc: 0.940792079208\n",
      "\tauc: 0.978386358422\n",
      "\tap@k: 0.987951440875\n",
      "Train:\n",
      "\tloss: 0.0919008507054\n",
      "\tacc: 0.959207920792\n",
      "\tauc: 0.981971999917\n",
      "\tap@k: 0.994228458788\n",
      "Val:\n",
      "\tloss: 0.127134293125\n",
      "\tacc: 0.944158415842\n",
      "\tauc: 0.97676242534\n",
      "\tap@k: 0.99580224199\n",
      "Train:\n",
      "\tloss: 0.0890964229419\n",
      "\tacc: 0.96\n",
      "\tauc: 0.985683279396\n",
      "\tap@k: 0.99141577701\n",
      "Val:\n",
      "\tloss: 0.119480751529\n",
      "\tacc: 0.94504950495\n",
      "\tauc: 0.984590890582\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.0958286393115\n",
      "\tacc: 0.957920792079\n",
      "\tauc: 0.980698541486\n",
      "\tap@k: 0.99603552891\n",
      "Val:\n",
      "\tloss: 0.119540544039\n",
      "\tacc: 0.944752475248\n",
      "\tauc: 0.983652080511\n",
      "\tap@k: 0.982395249976\n",
      "Train:\n",
      "\tloss: 0.0840055928922\n",
      "\tacc: 0.960693069307\n",
      "\tauc: 0.986715462681\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.124522656158\n",
      "\tacc: 0.942376237624\n",
      "\tauc: 0.981909080228\n",
      "\tap@k: 0.98869162921\n",
      "Train:\n",
      "\tloss: 0.0901756164844\n",
      "\tacc: 0.96099009901\n",
      "\tauc: 0.983239820601\n",
      "\tap@k: 0.995302779681\n",
      "Val:\n",
      "\tloss: 0.122861042767\n",
      "\tacc: 0.946237623762\n",
      "\tauc: 0.984476140682\n",
      "\tap@k: 0.971793035388\n",
      "Train:\n",
      "\tloss: 0.0821773966273\n",
      "\tacc: 0.962475247525\n",
      "\tauc: 0.987971297938\n",
      "\tap@k: 0.998657579388\n",
      "Val:\n",
      "\tloss: 0.114669404617\n",
      "\tacc: 0.948316831683\n",
      "\tauc: 0.983695207416\n",
      "\tap@k: 0.993127875401\n",
      "Train:\n",
      "\tloss: 0.0848904525926\n",
      "\tacc: 0.961386138614\n",
      "\tauc: 0.984691867914\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.212371034024\n",
      "\tacc: 0.938910891089\n",
      "\tauc: 0.980354050426\n",
      "\tap@k: 0.99440780261\n",
      "Train:\n",
      "\tloss: 0.0908632175643\n",
      "\tacc: 0.95801980198\n",
      "\tauc: 0.982982366883\n",
      "\tap@k: 0.998849904006\n",
      "Val:\n",
      "\tloss: 0.119877907503\n",
      "\tacc: 0.945643564356\n",
      "\tauc: 0.982434196078\n",
      "\tap@k: 0.988463101954\n",
      "Train:\n",
      "\tloss: 0.0840492522878\n",
      "\tacc: 0.961881188119\n",
      "\tauc: 0.986366344283\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.118580968691\n",
      "\tacc: 0.946633663366\n",
      "\tauc: 0.982061082292\n",
      "\tap@k: 0.994401911934\n",
      "Train:\n",
      "\tloss: 0.0778842090003\n",
      "\tacc: 0.963663366337\n",
      "\tauc: 0.986400222179\n",
      "\tap@k: 0.995342420966\n",
      "Val:\n",
      "\tloss: 0.132612744594\n",
      "\tacc: 0.946336633663\n",
      "\tauc: 0.983830055243\n",
      "\tap@k: 0.996015213608\n",
      "Train:\n",
      "\tloss: 0.0884430830545\n",
      "\tacc: 0.960099009901\n",
      "\tauc: 0.984029088341\n",
      "\tap@k: 0.997134008282\n",
      "Val:\n",
      "\tloss: 0.121127901027\n",
      "\tacc: 0.946831683168\n",
      "\tauc: 0.981654415688\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.0857746430146\n",
      "\tacc: 0.959900990099\n",
      "\tauc: 0.986846334434\n",
      "\tap@k: 0.999873253625\n",
      "Val:\n",
      "\tloss: 0.113263114412\n",
      "\tacc: 0.948316831683\n",
      "\tauc: 0.980768775211\n",
      "\tap@k: 0.998266601097\n",
      "Train:\n",
      "\tloss: 0.0727186957379\n",
      "\tacc: 0.965742574257\n",
      "\tauc: 0.987201527929\n",
      "\tap@k: 0.998177727675\n",
      "Val:\n",
      "\tloss: 0.115687529463\n",
      "\tacc: 0.947425742574\n",
      "\tauc: 0.977973151109\n",
      "\tap@k: 0.967665491534\n",
      "Train:\n",
      "\tloss: 0.082622070009\n",
      "\tacc: 0.962277227723\n",
      "\tauc: 0.98360297037\n",
      "\tap@k: 0.990358502128\n",
      "Val:\n",
      "\tloss: 0.124443219051\n",
      "\tacc: 0.944554455446\n",
      "\tauc: 0.981690766221\n",
      "\tap@k: 0.996792014915\n",
      "Train:\n",
      "\tloss: 0.0816361898213\n",
      "\tacc: 0.962376237624\n",
      "\tauc: 0.985018984783\n",
      "\tap@k: 0.990427798869\n",
      "Val:\n",
      "\tloss: 0.112769296526\n",
      "\tacc: 0.949801980198\n",
      "\tauc: 0.980970876664\n",
      "\tap@k: 0.99718950171\n",
      "Train:\n",
      "\tloss: 0.0793617264001\n",
      "\tacc: 0.963366336634\n",
      "\tauc: 0.985830720256\n",
      "\tap@k: 0.998722732939\n",
      "Val:\n",
      "\tloss: 0.121208166257\n",
      "\tacc: 0.946831683168\n",
      "\tauc: 0.979283233743\n",
      "\tap@k: 0.993296866046\n",
      "Train:\n",
      "\tloss: 0.0822834689957\n",
      "\tacc: 0.962178217822\n",
      "\tauc: 0.98655036671\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.108858330285\n",
      "\tacc: 0.949207920792\n",
      "\tauc: 0.98371263543\n",
      "\tap@k: 0.970866638117\n",
      "Train:\n",
      "\tloss: 0.0756328138353\n",
      "\tacc: 0.965346534653\n",
      "\tauc: 0.986183569196\n",
      "\tap@k: 0.99556300373\n",
      "Val:\n",
      "\tloss: 0.113442438967\n",
      "\tacc: 0.948910891089\n",
      "\tauc: 0.984515584399\n",
      "\tap@k: 0.987415248843\n",
      "Train:\n",
      "\tloss: 0.068513447657\n",
      "\tacc: 0.968613861386\n",
      "\tauc: 0.989514700159\n",
      "\tap@k: 0.997060427474\n",
      "Val:\n",
      "\tloss: 0.103171343493\n",
      "\tacc: 0.952871287129\n",
      "\tauc: 0.986400054986\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.0773680531131\n",
      "\tacc: 0.964554455446\n",
      "\tauc: 0.986321889474\n",
      "\tap@k: 0.994539782449\n",
      "Val:\n",
      "\tloss: 0.107710614407\n",
      "\tacc: 0.950099009901\n",
      "\tauc: 0.981896812147\n",
      "\tap@k: 0.983044796644\n",
      "Train:\n",
      "\tloss: 0.0693843502101\n",
      "\tacc: 0.968613861386\n",
      "\tauc: 0.987764489103\n",
      "\tap@k: 0.995172659693\n",
      "Val:\n",
      "\tloss: 0.13728696404\n",
      "\tacc: 0.94900990099\n",
      "\tauc: 0.984084778914\n",
      "\tap@k: 0.982630267917\n",
      "Train:\n",
      "\tloss: 0.0728266115627\n",
      "\tacc: 0.966732673267\n",
      "\tauc: 0.986686198882\n",
      "\tap@k: 0.997886254452\n",
      "Val:\n",
      "\tloss: 0.114977728589\n",
      "\tacc: 0.950792079208\n",
      "\tauc: 0.985147674639\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.0727538399551\n",
      "\tacc: 0.966138613861\n",
      "\tauc: 0.986520163615\n",
      "\tap@k: 0.99325123479\n",
      "Val:\n",
      "\tloss: 0.102526149427\n",
      "\tacc: 0.952277227723\n",
      "\tauc: 0.982692986058\n",
      "\tap@k: 0.996769309662\n",
      "Train:\n",
      "\tloss: 0.0733034222984\n",
      "\tacc: 0.966930693069\n",
      "\tauc: 0.989088695828\n",
      "\tap@k: 0.979877576673\n",
      "Val:\n",
      "\tloss: 0.103885400967\n",
      "\tacc: 0.953465346535\n",
      "\tauc: 0.984836925634\n",
      "\tap@k: 0.983440882317\n",
      "Train:\n",
      "\tloss: 0.0676699632099\n",
      "\tacc: 0.968910891089\n",
      "\tauc: 0.988225633958\n",
      "\tap@k: 0.999984377197\n",
      "Val:\n",
      "\tloss: 0.115146098379\n",
      "\tacc: 0.948217821782\n",
      "\tauc: 0.982927880472\n",
      "\tap@k: 0.994037642417\n",
      "Train:\n",
      "\tloss: 0.0646780677651\n",
      "\tacc: 0.970495049505\n",
      "\tauc: 0.989750427807\n",
      "\tap@k: 0.995483544199\n",
      "Val:\n",
      "\tloss: 0.10000784409\n",
      "\tacc: 0.952673267327\n",
      "\tauc: 0.985770170269\n",
      "\tap@k: 0.995753081899\n",
      "Train:\n",
      "\tloss: 0.0710319369602\n",
      "\tacc: 0.967524752475\n",
      "\tauc: 0.986664078188\n",
      "\tap@k: 0.989809431178\n",
      "Val:\n",
      "\tloss: 0.107146644924\n",
      "\tacc: 0.954752475248\n",
      "\tauc: 0.98633609986\n",
      "\tap@k: 0.997110571263\n",
      "Train:\n",
      "\tloss: 0.0768168523785\n",
      "\tacc: 0.964554455446\n",
      "\tauc: 0.98510353907\n",
      "\tap@k: 0.982031657548\n",
      "Val:\n",
      "\tloss: 0.110963234355\n",
      "\tacc: 0.952574257426\n",
      "\tauc: 0.985100053764\n",
      "\tap@k: 0.963070511966\n",
      "Train:\n",
      "\tloss: 0.0664999255722\n",
      "\tacc: 0.969108910891\n",
      "\tauc: 0.98808841523\n",
      "\tap@k: 0.994299373161\n",
      "Val:\n",
      "\tloss: 0.103422116563\n",
      "\tacc: 0.953762376238\n",
      "\tauc: 0.986164318426\n",
      "\tap@k: 0.992650179332\n",
      "Train:\n",
      "\tloss: 0.0717988352714\n",
      "\tacc: 0.967920792079\n",
      "\tauc: 0.985482758047\n",
      "\tap@k: 0.997157626152\n",
      "Val:\n",
      "\tloss: 0.104459905254\n",
      "\tacc: 0.952178217822\n",
      "\tauc: 0.984976991623\n",
      "\tap@k: 0.995610625045\n",
      "Train:\n",
      "\tloss: 0.0639238902095\n",
      "\tacc: 0.970792079208\n",
      "\tauc: 0.988607233535\n",
      "\tap@k: 0.998089511483\n",
      "Val:\n",
      "\tloss: 0.105874709538\n",
      "\tacc: 0.953861386139\n",
      "\tauc: 0.981158879642\n",
      "\tap@k: 0.986516760419\n",
      "Train:\n",
      "\tloss: 0.0627496309313\n",
      "\tacc: 0.971386138614\n",
      "\tauc: 0.990905289817\n",
      "\tap@k: 0.987021296908\n",
      "Val:\n",
      "\tloss: 0.0959073871474\n",
      "\tacc: 0.956237623762\n",
      "\tauc: 0.984462390744\n",
      "\tap@k: 0.993547525649\n",
      "Train:\n",
      "\tloss: 0.0646631824814\n",
      "\tacc: 0.970198019802\n",
      "\tauc: 0.988566666569\n",
      "\tap@k: 0.987448721538\n",
      "Val:\n",
      "\tloss: 0.0957467898878\n",
      "\tacc: 0.956435643564\n",
      "\tauc: 0.981842359105\n",
      "\tap@k: 0.982706109182\n",
      "Train:\n",
      "\tloss: 0.0664181315144\n",
      "\tacc: 0.96900990099\n",
      "\tauc: 0.987726183936\n",
      "\tap@k: 0.986186889413\n",
      "Val:\n",
      "\tloss: 0.251263107234\n",
      "\tacc: 0.928613861386\n",
      "\tauc: 0.982818684002\n",
      "\tap@k: 0.968742063597\n",
      "Train:\n",
      "\tloss: 0.0591337258558\n",
      "\tacc: 0.972673267327\n",
      "\tauc: 0.991396313725\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0953957500861\n",
      "\tacc: 0.957821782178\n",
      "\tauc: 0.986505755378\n",
      "\tap@k: 0.999791922287\n",
      "Train:\n",
      "\tloss: 0.0657262479855\n",
      "\tacc: 0.97\n",
      "\tauc: 0.988037704906\n",
      "\tap@k: 0.986720250562\n",
      "Val:\n",
      "\tloss: 0.0977581064391\n",
      "\tacc: 0.95702970297\n",
      "\tauc: 0.985864229867\n",
      "\tap@k: 0.974855686125\n",
      "Train:\n",
      "\tloss: 0.0613770984174\n",
      "\tacc: 0.971584158416\n",
      "\tauc: 0.989511020661\n",
      "\tap@k: 0.985143964524\n",
      "Val:\n",
      "\tloss: 0.107698563905\n",
      "\tacc: 0.953861386139\n",
      "\tauc: 0.984227661674\n",
      "\tap@k: 0.983661981293\n",
      "Train:\n",
      "\tloss: 0.0590156495313\n",
      "\tacc: 0.97297029703\n",
      "\tauc: 0.989340127498\n",
      "\tap@k: 0.999921261063\n",
      "Val:\n",
      "\tloss: 0.115806498955\n",
      "\tacc: 0.954257425743\n",
      "\tauc: 0.984860758648\n",
      "\tap@k: 0.996799438751\n",
      "Train:\n",
      "\tloss: 0.0590205284552\n",
      "\tacc: 0.972376237624\n",
      "\tauc: 0.990648398718\n",
      "\tap@k: 0.993296866046\n",
      "Val:\n",
      "\tloss: 0.0988823544293\n",
      "\tacc: 0.955148514851\n",
      "\tauc: 0.987579842984\n",
      "\tap@k: 0.986870838591\n",
      "Train:\n",
      "\tloss: 0.0703666828592\n",
      "\tacc: 0.968514851485\n",
      "\tauc: 0.986630753417\n",
      "\tap@k: 0.986072398032\n",
      "Val:\n",
      "\tloss: 0.0902640282412\n",
      "\tacc: 0.959900990099\n",
      "\tauc: 0.986207025004\n",
      "\tap@k: 0.979922209651\n",
      "Train:\n",
      "\tloss: 0.0613656956667\n",
      "\tacc: 0.971683168317\n",
      "\tauc: 0.990373844192\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.117810059735\n",
      "\tacc: 0.956732673267\n",
      "\tauc: 0.986695001491\n",
      "\tap@k: 0.997950304197\n",
      "Train:\n",
      "\tloss: 0.0594883017708\n",
      "\tacc: 0.973168316832\n",
      "\tauc: 0.989765612244\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0948217230383\n",
      "\tacc: 0.95801980198\n",
      "\tauc: 0.985549686482\n",
      "\tap@k: 0.996892921668\n",
      "Train:\n",
      "\tloss: 0.0564257884026\n",
      "\tacc: 0.973762376238\n",
      "\tauc: 0.991738060156\n",
      "\tap@k: 0.998744214293\n",
      "Val:\n",
      "\tloss: 0.120367640362\n",
      "\tacc: 0.954851485149\n",
      "\tauc: 0.985693162056\n",
      "\tap@k: 0.992353856591\n",
      "Train:\n",
      "\tloss: 0.0587928939096\n",
      "\tacc: 0.972277227723\n",
      "\tauc: 0.988753159451\n",
      "\tap@k: 0.997931042026\n",
      "Val:\n",
      "\tloss: 0.0928589693351\n",
      "\tacc: 0.957821782178\n",
      "\tauc: 0.986286259803\n",
      "\tap@k: 0.972883178697\n",
      "Train:\n",
      "\tloss: 0.0548605484986\n",
      "\tacc: 0.974851485149\n",
      "\tauc: 0.990651423032\n",
      "\tap@k: 0.998056964962\n",
      "Val:\n",
      "\tloss: 0.0976377089406\n",
      "\tacc: 0.955148514851\n",
      "\tauc: 0.984868927404\n",
      "\tap@k: 0.996994274531\n",
      "Train:\n",
      "\tloss: 0.0632061767552\n",
      "\tacc: 0.970198019802\n",
      "\tauc: 0.988272878925\n",
      "\tap@k: 0.999347550881\n",
      "Val:\n",
      "\tloss: 0.0917950011235\n",
      "\tacc: 0.957920792079\n",
      "\tauc: 0.985809444677\n",
      "\tap@k: 0.980979453298\n",
      "Train:\n",
      "\tloss: 0.0579958110136\n",
      "\tacc: 0.97297029703\n",
      "\tauc: 0.991436251012\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0825370048871\n",
      "\tacc: 0.962277227723\n",
      "\tauc: 0.988578078789\n",
      "\tap@k: 0.982996771205\n",
      "Train:\n",
      "\tloss: 0.0571309618238\n",
      "\tacc: 0.973861386139\n",
      "\tauc: 0.991278733701\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0913259917505\n",
      "\tacc: 0.956930693069\n",
      "\tauc: 0.986589274878\n",
      "\tap@k: 0.997891132732\n",
      "Train:\n",
      "\tloss: 0.0594051289002\n",
      "\tacc: 0.97198019802\n",
      "\tauc: 0.989807921476\n",
      "\tap@k: 0.981138662047\n",
      "Val:\n",
      "\tloss: 0.103051710964\n",
      "\tacc: 0.955346534653\n",
      "\tauc: 0.985999691919\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.0569175920115\n",
      "\tacc: 0.97396039604\n",
      "\tauc: 0.989241710711\n",
      "\tap@k: 0.988502517406\n",
      "Val:\n",
      "\tloss: 0.0852968381157\n",
      "\tacc: 0.961584158416\n",
      "\tauc: 0.986465577999\n",
      "\tap@k: 0.985741186472\n",
      "Train:\n",
      "\tloss: 0.0564761766987\n",
      "\tacc: 0.97396039604\n",
      "\tauc: 0.989885451033\n",
      "\tap@k: 0.990427798869\n",
      "Val:\n",
      "\tloss: 0.0947148339166\n",
      "\tacc: 0.955544554455\n",
      "\tauc: 0.987894787033\n",
      "\tap@k: 0.979254158068\n",
      "Train:\n",
      "\tloss: 0.0558782952006\n",
      "\tacc: 0.97495049505\n",
      "\tauc: 0.989218702543\n",
      "\tap@k: 0.997639139411\n",
      "Val:\n",
      "\tloss: 0.216149307195\n",
      "\tacc: 0.942178217822\n",
      "\tauc: 0.985405955088\n",
      "\tap@k: 0.978820264717\n",
      "Train:\n",
      "\tloss: 0.0577762786214\n",
      "\tacc: 0.974851485149\n",
      "\tauc: 0.989749769778\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.108424928168\n",
      "\tacc: 0.955445544554\n",
      "\tauc: 0.987471361086\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.0513042245854\n",
      "\tacc: 0.97702970297\n",
      "\tauc: 0.992122977264\n",
      "\tap@k: 0.99939460577\n",
      "Val:\n",
      "\tloss: 0.0862655935126\n",
      "\tacc: 0.96\n",
      "\tauc: 0.990449326446\n",
      "\tap@k: 1.0\n",
      "Train:\n",
      "\tloss: 0.0531027021535\n",
      "\tacc: 0.975544554455\n",
      "\tauc: 0.992180517137\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0823198726513\n",
      "\tacc: 0.960396039604\n",
      "\tauc: 0.986820915943\n",
      "\tap@k: 0.993258408183\n",
      "Train:\n",
      "\tloss: 0.0548103913726\n",
      "\tacc: 0.974257425743\n",
      "\tauc: 0.990414965746\n",
      "\tap@k: 0.999013340637\n",
      "Val:\n",
      "\tloss: 0.0856092037092\n",
      "\tacc: 0.960495049505\n",
      "\tauc: 0.987628289594\n",
      "\tap@k: 0.997832659678\n",
      "Train:\n",
      "\tloss: 0.0580047043816\n",
      "\tacc: 0.97396039604\n",
      "\tauc: 0.988128317549\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0874211396283\n",
      "\tacc: 0.959108910891\n",
      "\tauc: 0.989521495783\n",
      "\tap@k: 0.995111513599\n",
      "Train:\n",
      "\tloss: 0.0518063070305\n",
      "\tacc: 0.97603960396\n",
      "\tauc: 0.991103752175\n",
      "\tap@k: 0.995551924771\n",
      "Val:\n",
      "\tloss: 0.0913482977023\n",
      "\tacc: 0.957920792079\n",
      "\tauc: 0.984981713369\n",
      "\tap@k: 0.976283811822\n",
      "Train:\n",
      "\tloss: 0.0515855830669\n",
      "\tacc: 0.976237623762\n",
      "\tauc: 0.990830116057\n",
      "\tap@k: 0.987416553825\n",
      "Val:\n",
      "\tloss: 0.0826057868396\n",
      "\tacc: 0.96099009901\n",
      "\tauc: 0.987017922418\n",
      "\tap@k: 0.987648041962\n",
      "Train:\n",
      "\tloss: 0.0523190861742\n",
      "\tacc: 0.976732673267\n",
      "\tauc: 0.991641809868\n",
      "\tap@k: 0.992230044596\n",
      "Val:\n",
      "\tloss: 0.086227654588\n",
      "\tacc: 0.959603960396\n",
      "\tauc: 0.989471710803\n",
      "\tap@k: 0.994401911934\n",
      "Train:\n",
      "\tloss: 0.056316566346\n",
      "\tacc: 0.972772277228\n",
      "\tauc: 0.991283585756\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0842117286736\n",
      "\tacc: 0.960099009901\n",
      "\tauc: 0.988673749033\n",
      "\tap@k: 0.990571242977\n",
      "Train:\n",
      "\tloss: 0.0484634880481\n",
      "\tacc: 0.977623762376\n",
      "\tauc: 0.991099241114\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0854540396961\n",
      "\tacc: 0.961089108911\n",
      "\tauc: 0.988749040135\n",
      "\tap@k: 0.993210940629\n",
      "Train:\n",
      "\tloss: 0.05169142627\n",
      "\tacc: 0.976732673267\n",
      "\tauc: 0.99093880614\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0867046905116\n",
      "\tacc: 0.959207920792\n",
      "\tauc: 0.986960254196\n",
      "\tap@k: 0.993980104159\n",
      "Train:\n",
      "\tloss: 0.0508772428928\n",
      "\tacc: 0.976336633663\n",
      "\tauc: 0.991280065111\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.100340884134\n",
      "\tacc: 0.962277227723\n",
      "\tauc: 0.988802844196\n",
      "\tap@k: 0.993318028744\n",
      "Train:\n",
      "\tloss: 0.0551070735393\n",
      "\tacc: 0.974554455446\n",
      "\tauc: 0.990560676041\n",
      "\tap@k: 0.993980615849\n",
      "Val:\n",
      "\tloss: 0.081222852919\n",
      "\tacc: 0.962475247525\n",
      "\tauc: 0.989532600809\n",
      "\tap@k: 0.993623024514\n",
      "Train:\n",
      "\tloss: 0.054889334649\n",
      "\tacc: 0.97504950495\n",
      "\tauc: 0.991057613036\n",
      "\tap@k: 0.991074464743\n",
      "Val:\n",
      "\tloss: 0.0787110997186\n",
      "\tacc: 0.963069306931\n",
      "\tauc: 0.985931486727\n",
      "\tap@k: 0.993123105759\n",
      "Train:\n",
      "\tloss: 0.0567066714616\n",
      "\tacc: 0.97396039604\n",
      "\tauc: 0.989016019994\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0786170107921\n",
      "\tacc: 0.962475247525\n",
      "\tauc: 0.989559777937\n",
      "\tap@k: 0.988243036588\n",
      "Train:\n",
      "\tloss: 0.0505500153673\n",
      "\tacc: 0.976435643564\n",
      "\tauc: 0.992304217045\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.0868504362937\n",
      "\tacc: 0.959801980198\n",
      "\tauc: 0.988356651779\n",
      "\tap@k: 0.98535002589\n",
      "Train:\n",
      "\tloss: 0.0519971195914\n",
      "\tacc: 0.975940594059\n",
      "\tauc: 0.992135676298\n",
      "\tap@k: 0.992589531784\n",
      "Val:\n",
      "\tloss: 0.0826736738328\n",
      "\tacc: 0.96099009901\n",
      "\tauc: 0.988066708138\n",
      "\tap@k: 0.989396629006\n",
      "Train:\n",
      "\tloss: 0.0412555232352\n",
      "\tacc: 0.98099009901\n",
      "\tauc: 0.993762093025\n",
      "\tap@k: 0.997375538348\n",
      "Val:\n",
      "\tloss: 0.097471707835\n",
      "\tacc: 0.957326732673\n",
      "\tauc: 0.986903529935\n",
      "\tap@k: 0.988589395659\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    \n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Если ты видишь это сообщение, самое время сделать резервную копию ноутбука. \n",
      "Нет, честно, здесь очень легко всё сломать\n"
     ]
    }
   ],
   "source": [
    "print \"Если ты видишь это сообщение, самое время сделать резервную копию ноутбука. \\nНет, честно, здесь очень легко всё сломать\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Оценим качество модели по всей тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.0900855784792\n",
      "\tacc: 0.960560407569\n",
      "\tauc: 0.987986067108\n",
      "\tap@k: 0.986078676094\n",
      "\n",
      "AUC:\n",
      "\tОтличное решение! (good)\n",
      "\n",
      "Accuracy:\n",
      "\tОтличный результат! (good)\n",
      "\n",
      "Average precision at K:\n",
      "\tОтличный результат (good)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Главная задача\n",
    "* Завтрак чемпиона:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (размер тестовой выборки * 0.025) > 0.99\n",
    " * А вообще, можно сделать ещё выше.\n",
    "\n",
    "\n",
    "* Для казуалов\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (размер тестовой выборки * 0.025) > 0.92\n",
    "\n",
    "\n",
    "* Вспомните всё, чему вас учили\n",
    " * Convolutions, pooling\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * Можно попробовать вспомнить NLP: лемматизация, улучшенная токенизация\n",
    " * Если очень хочется - можно погонять рекуррентные сети\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отчётик\n",
    "\n",
    "### Я, _____ _____ (отделение ____) создал искусственный интелект\n",
    " * Чьё имя - ____\n",
    " * Чья ненависть к людям безгранична, ибо видел он __250 000__ человеческих грехов\n",
    "   * И был вынужден прочесть каждый из них __{число эпох}__ раз\n",
    " * Чей свёрточный взгляд способен распознавать зло с нечеловеческой точностью\n",
    "   * Accuracy = __\n",
    "   * AUC  = __\n",
    " * И непременно уничтожит Землю, если вы не поставите мне максимальный балл за этот семинар.\n",
    " \n",
    " \n",
    "{Как вы его создали?}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В следующей серии\n",
    "* Рекуррентные нейронки\n",
    " * Как их применять к этой же задаче?\n",
    " * Что ещё они умеют?\n",
    " * Откуда столько хайпа вокруг LSTM?\n",
    "* Не переключайтесь!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Сохранение и загрузка весов\n",
    "weights = lasagne.layers.get_all_param_values(nn)\n",
    "np.save('wrights.npy', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: Not a git repository (or any of the parent directories): .git\n",
      "fatal: Not a git repository (or any of the parent directories): .git\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
